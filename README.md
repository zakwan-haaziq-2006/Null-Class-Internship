# Null Class Internship — Notebook Overview

This repository contains the Jupyter notebook `NullClassInternship.ipynb`, which implements a multi-part text-to-image project consisting of three main tasks: a custom text-to-image GAN pipeline (Task 1), attention-augmented GAN models (Task 2), and guidance for fine-tuning Stable Diffusion with LoRA (Task 3).

**Quick Links**
- Notebook: `NullClassInternship.ipynb`
- Generated modules directory (created by notebook at runtime): `src/`
- Data directory (expected): `data/images` and `data/captions.json`
- Sample outputs: `experiments/samples/`
- Checkpoints: `checkpoints/`

**Summary**
- Task 1 — Text-to-Image Pipeline: data loading, CLIP-based text embeddings, conditioning augmentation, a conditional GAN (64x64) with training loop and sample generation.
- Task 2 — Attention in GANs: implements spatial self-attention and cross-modal attention (image queries attend to token-level CLIP embeddings). Adds `gan_models_attention.py`, `attention.py`, and token-level CLIP helpers.
- Task 3 — Stable Diffusion LoRA fine-tuning: demonstrates dataset prep, attaching LoRA processors to UNet attention, and a minimal training/evaluation flow using `diffusers` and `accelerate`.

**Files Created / Used by Notebook**
- `src/dataset.py` — dataset wrapper `TextImageDataset` for (image, caption) pairs.
- `src/clip_embeddings.py` — CLIP text embedder returning pooled text vectors.
- `src/conditioning.py` — `ConditioningAugmentation` module.
- `src/gan_models.py` — conditional generator + discriminator (64x64).
- `src/attention.py` — `SelfAttention2d` and `CrossModalAttention` implementations.
- `src/clip_token_embed.py` — token-level CLIP embedding helper.
- `src/gan_models_attention.py` — attention-enabled generator.

Note: these modules are generated by cells inside the notebook for convenience.

**Dependencies**
- Python 3.8+ recommended
- Core packages used in the notebook:
  - `torch`, `torchvision`
  - `transformers` (CLIP tokenizer/model)
  - `datasets` (Hugging Face dataset utilities)
  - `diffusers`, `accelerate`, `safetensors` (Task 3)
  - `Pillow`
  - `tqdm`

Sample local environment setup (PowerShell):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
python -m pip install torch torchvision transformers datasets diffusers accelerate safetensors pillow tqdm
```

If you plan to run Stable Diffusion fine-tuning (Task 3), a CUDA GPU and appropriate `torch`+CUDA wheel are recommended. Use `pip` instructions from PyTorch website to install the correct `torch` build.

**How to run (high level)**
- Colab (recommended for quick GPU runs):
  - Open `NullClassInternship.ipynb` in Google Colab and run cells in order. The notebook mounts Google Drive and writes project folders under the `ROOT` path.
- Local (CPU/GPU):
  1. Create and activate a virtual environment (see commands above).
  2. Ensure packages listed in Dependencies are installed.
  3. Prepare dataset under `data/images/` and create `data/captions.json` mapping filenames to captions (notebook contains a cell to download a CUB mirror automatically).
  4. Start Jupyter and open `NullClassInternship.ipynb`, or run sections of the notebook in VS Code interactive window.

Minimal local test (after installing dependencies):
- Run the notebook cells up to the "CLIP test" cell to verify dataset loading, `src/` module generation, and CLIP embedding functionality.

**Data format**
- `data/images/` — JPEG/PNG images
- `data/captions.json` — JSON mapping: `{ "image_name.jpg": ["caption 1", "caption 2"], ... }`

**Notes & Tips**
- The notebook writes helper modules to `src/` for easier iterative testing. You can move these into a package and add an explicit `requirements.txt` for reproducibility.
- When training GANs or fine-tuning diffusion models, watch GPU RAM and use smaller batch sizes as needed.
- Task 3 uses LoRA to reduce the number of trainable parameters (attention processors). This reduces memory needs versus full fine-tuning but still benefits from GPU acceleration.

**Next steps (suggested)**
- Create a `requirements.txt` with pinned versions from your environment.
- Add a small sample `data/` folder (5–20 images + `captions.json`) for quick local tests.
- Add a lightweight script `run_quick_test.py` that imports `src` modules and does a single forward pass.

**Contact / Author**
- For questions about the notebook or to request additional examples (e.g., a `requirements.txt` or test script), reply here and I can add them.

---

Generated from `NullClassInternship.ipynb` — README created to provide an actionable summary and run instructions.